{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:49.986292Z",
     "start_time": "2025-04-20T12:23:49.771131Z"
    }
   },
   "cell_type": "code",
   "source": "from Utils import accuracy, load_feature_data, load_label_data",
   "id": "ff5f9dde50c4bae6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:50.613035Z",
     "start_time": "2025-04-20T12:23:50.607152Z"
    }
   },
   "source": [
    "# import numpy as np\n",
    "#\n",
    "# class DecisionStump:\n",
    "#     \"\"\"\n",
    "#     决策树桩实现 - 一个简单的单层决策树\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         self.feature_idx = None  # 用于分类的特征索引\n",
    "#         self.threshold = None    # 分类阈值\n",
    "#         self.polarity = 1        # 极性，决定符号方向（+1 或 -1）\n",
    "#         self.alpha = None        # 该分类器的权重\n",
    "#\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"根据选定的特征和阈值预测样本类别\"\"\"\n",
    "#         n_samples = X.shape[0]\n",
    "#         feature_values = X[:, self.feature_idx]\n",
    "#\n",
    "#         # 初始化预测结果\n",
    "#         predictions = np.ones(n_samples)\n",
    "#\n",
    "#         # 应用阈值和极性进行分类\n",
    "#         if self.polarity == 1:\n",
    "#             predictions[feature_values < self.threshold] = -1\n",
    "#         else:\n",
    "#             predictions[feature_values >= self.threshold] = -1\n",
    "#\n",
    "#         return predictions\n",
    "#\n",
    "#\n",
    "# class MyAdaBoost:\n",
    "#     \"\"\"\n",
    "#     AdaBoost分类器实现\n",
    "#     \"\"\"\n",
    "#     def __init__(self, n_estimators=50):\n",
    "#         \"\"\"\n",
    "#         初始化AdaBoost分类器\n",
    "#\n",
    "#         参数:\n",
    "#             n_estimators: 弱分类器(决策树桩)的数量\n",
    "#         \"\"\"\n",
    "#         self.n_estimators = n_estimators\n",
    "#         self.estimators = []\n",
    "#\n",
    "#     def fit(self, X, y):\n",
    "#         \"\"\"\n",
    "#         训练AdaBoost分类器\n",
    "#\n",
    "#         参数:\n",
    "#             X: 形状为[n_samples, n_features]的训练特征\n",
    "#             y: 形状为[n_samples]的训练标签，值为+1或-1\n",
    "#\n",
    "#         返回:\n",
    "#             self: 训练后的分类器\n",
    "#         \"\"\"\n",
    "#         n_samples, n_features = X.shape\n",
    "#\n",
    "#         # 初始化样本权重为均匀分布\n",
    "#         weights = np.ones(n_samples) / n_samples\n",
    "#\n",
    "#         # 逐步训练弱分类器\n",
    "#         for _ in range(self.n_estimators):\n",
    "#             # 创建并训练决策树桩\n",
    "#             stump = DecisionStump()\n",
    "#             min_error = float('inf')\n",
    "#\n",
    "#             # 对每个特征尝试找到最佳的阈值和极性\n",
    "#             for feature_idx in range(n_features):\n",
    "#                 feature_values = X[:, feature_idx]\n",
    "#                 thresholds = np.unique(feature_values)\n",
    "#\n",
    "#                 # 对每个可能的阈值进行评估\n",
    "#                 for threshold in thresholds:\n",
    "#                     # 尝试两种极性\n",
    "#                     for polarity in [1, -1]:\n",
    "#                         # 预测结果\n",
    "#                         predictions = np.ones(n_samples)\n",
    "#                         if polarity == 1:\n",
    "#                             predictions[feature_values < threshold] = -1\n",
    "#                         else:\n",
    "#                             predictions[feature_values >= threshold] = -1\n",
    "#\n",
    "#                         # 计算加权错误率\n",
    "#                         misclassified = predictions != y\n",
    "#                         error = np.sum(weights[misclassified])\n",
    "#\n",
    "#                         # 更新找到更好的参数组合\n",
    "#                         if error < min_error:\n",
    "#                             min_error = error\n",
    "#                             stump.feature_idx = feature_idx\n",
    "#                             stump.threshold = threshold\n",
    "#                             stump.polarity = polarity\n",
    "#\n",
    "#             # 防止除零错误\n",
    "#             epsilon = 1e-10\n",
    "#\n",
    "#             # 计算弱分类器权重\n",
    "#             stump.alpha = 0.5 * np.log((1.0 - min_error + epsilon) / (min_error + epsilon))\n",
    "#\n",
    "#             # 获取当前弱分类器的预测\n",
    "#             predictions = stump.predict(X)\n",
    "#\n",
    "#             # 更新样本权重\n",
    "#             weights *= np.exp(-stump.alpha * y * predictions)\n",
    "#             # 归一化权重\n",
    "#             weights /= np.sum(weights)\n",
    "#\n",
    "#             # 保存弱分类器\n",
    "#             self.estimators.append(stump)\n",
    "#\n",
    "#             # 如果错误率为0，提前结束训练\n",
    "#             if min_error == 0:\n",
    "#                 break\n",
    "#\n",
    "#         return self\n",
    "#\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         使用AdaBoost集成进行预测\n",
    "#\n",
    "#         参数:\n",
    "#             X: 形状为[n_samples, n_features]的测试特征\n",
    "#\n",
    "#         返回:\n",
    "#             预测的类别标签，值为+1或-1\n",
    "#         \"\"\"\n",
    "#         n_samples = X.shape[0]\n",
    "#         y_pred = np.zeros(n_samples)\n",
    "#\n",
    "#         # 计算所有弱分类器的加权投票\n",
    "#         for stump in self.estimators:\n",
    "#             y_pred += stump.alpha * stump.predict(X)\n",
    "#\n",
    "#         # 返回集成结果的符号\n",
    "#         return np.sign(y_pred)\n",
    "#\n",
    "#     def score(self, X, y):\n",
    "#         \"\"\"\n",
    "#         计算分类准确率\n",
    "#\n",
    "#         参数:\n",
    "#             X: 形状为[n_samples, n_features]的测试特征\n",
    "#             y: 形状为[n_samples]的真实标签\n",
    "#\n",
    "#         返回:\n",
    "#             分类准确率（0到1之间）\n",
    "#         \"\"\"\n",
    "#         y_pred = self.predict(X)\n",
    "#         return np.mean(y_pred == y)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:51.917572Z",
     "start_time": "2025-04-20T12:23:51.914564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_feature = load_feature_data('data.csv')\n",
    "# train_label = load_label_data('targets.csv')\n",
    "#\n",
    "# model = MyAdaBoost()\n",
    "# train_label =np.array([1 if label == 1 else -1 for label in train_label])\n",
    "# model.fit(train_feature, train_label)\n",
    "# y_pred = model.predict(train_feature)\n",
    "# print(\"Predictions:\", y_pred)\n",
    "# print(\"Accuracy:\", model.score(train_feature, train_label))\n",
    "# print(\"Acc:\", accuracy(train_label, y_pred))"
   ],
   "id": "b860ab926ed18572",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:52.647004Z",
     "start_time": "2025-04-20T12:23:52.644911Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "47706c7272d33ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:53.314579Z",
     "start_time": "2025-04-20T12:23:53.312233Z"
    }
   },
   "cell_type": "code",
   "source": "# print(accuracy(train_label, y_pred))",
   "id": "f677fdec5aa4d1ad",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:53.958042Z",
     "start_time": "2025-04-20T12:23:53.955074Z"
    }
   },
   "cell_type": "code",
   "source": "# print(train_label)",
   "id": "81b6377a72d9ea1f",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:54.469133Z",
     "start_time": "2025-04-20T12:23:54.466420Z"
    }
   },
   "cell_type": "code",
   "source": "# print(y_pred)",
   "id": "7f2486edcb6b0ec2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:55.227756Z",
     "start_time": "2025-04-20T12:23:55.221667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "#\n",
    "# class BaseWeakLearner:\n",
    "#     \"\"\"弱学习器的基类，定义基本接口\"\"\"\n",
    "#     def fit(self, X, y, weights):\n",
    "#         \"\"\"使用加权样本进行训练\"\"\"\n",
    "#         pass\n",
    "#\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"对样本进行预测\"\"\"\n",
    "#         pass\n",
    "#\n",
    "#\n",
    "# class DecisionStump(BaseWeakLearner):\n",
    "#     \"\"\"决策树桩实现 - 一个简单的单层决策树\"\"\"\n",
    "#\n",
    "#     def __init__(self):\n",
    "#         self.feature_idx = None  # 用于分类的特征索引\n",
    "#         self.threshold = None    # 分类阈值\n",
    "#         self.polarity = 1        # 极性，决定符号方向（+1 或 -1）\n",
    "#         self.alpha = None        # 该分类器在集成中的权重\n",
    "#\n",
    "#     def fit(self, X, y, weights):\n",
    "#         \"\"\"\n",
    "#         训练决策树桩，在加权数据上找到最佳分割点\n",
    "#\n",
    "#         参数:\n",
    "#             X: 形状为[n_samples, n_features]的训练特征\n",
    "#             y: 形状为[n_samples]的训练标签，值为+1或-1\n",
    "#             weights: 样本权重\n",
    "#\n",
    "#         返回:\n",
    "#             min_error: 最小的加权错误率\n",
    "#         \"\"\"\n",
    "#         n_samples, n_features = X.shape\n",
    "#         min_error = float('inf')\n",
    "#\n",
    "#         # 对每个特征尝试找到最佳的阈值和极性\n",
    "#         for feature_idx in range(n_features):\n",
    "#             feature_values = X[:, feature_idx]\n",
    "#             thresholds = np.unique(feature_values)\n",
    "#\n",
    "#             # 对每个可能的阈值进行评估\n",
    "#             for threshold in thresholds:\n",
    "#                 # 尝试两种极性\n",
    "#                 for polarity in [1, -1]:\n",
    "#                     # 预测结果\n",
    "#                     predictions = np.ones(n_samples)\n",
    "#                     if polarity == 1:\n",
    "#                         predictions[feature_values < threshold] = -1\n",
    "#                     else:\n",
    "#                         predictions[feature_values >= threshold] = -1\n",
    "#\n",
    "#                     # 计算加权错误率\n",
    "#                     misclassified = predictions != y\n",
    "#                     error = np.sum(weights[misclassified])\n",
    "#\n",
    "#                     # 更新找到更好的参数组合\n",
    "#                     if error < min_error:\n",
    "#                         min_error = error\n",
    "#                         self.feature_idx = feature_idx\n",
    "#                         self.threshold = threshold\n",
    "#                         self.polarity = polarity\n",
    "#\n",
    "#         return min_error\n",
    "#\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"根据选定的特征和阈值预测样本类别\"\"\"\n",
    "#         n_samples = X.shape[0]\n",
    "#         feature_values = X[:, self.feature_idx]\n",
    "#\n",
    "#         # 初始化预测结果\n",
    "#         predictions = np.ones(n_samples)\n",
    "#\n",
    "#         # 应用阈值和极性进行分类\n",
    "#         if self.polarity == 1:\n",
    "#             predictions[feature_values < self.threshold] = -1\n",
    "#         else:\n",
    "#             predictions[feature_values >= self.threshold] = -1\n",
    "#\n",
    "#         return predictions\n",
    "#\n",
    "#\n",
    "# class MyAdaBoost:\n",
    "#     \"\"\"AdaBoost分类器实现\"\"\"\n",
    "#\n",
    "#     def __init__(self, base_learner_class=DecisionStump, n_estimators=50):\n",
    "#         \"\"\"\n",
    "#         初始化AdaBoost分类器\n",
    "#\n",
    "#         参数:\n",
    "#             base_learner_class: 基学习器类（默认为决策树桩）\n",
    "#             n_estimators: 弱分类器的数量\n",
    "#         \"\"\"\n",
    "#         self.base_learner_class = base_learner_class\n",
    "#         self.n_estimators = n_estimators\n",
    "#         self.estimators = []\n",
    "#\n",
    "#     def fit(self, X, y):\n",
    "#         \"\"\"\n",
    "#         训练AdaBoost分类器\n",
    "#\n",
    "#         参数:\n",
    "#             X: 形状为[n_samples, n_features]的训练特征\n",
    "#             y: 形状为[n_samples]的训练标签，值为+1或-1\n",
    "#\n",
    "#         返回:\n",
    "#             self: 训练后的分类器\n",
    "#         \"\"\"\n",
    "#         n_samples = X.shape[0]\n",
    "#\n",
    "#         # 初始化样本权重为均匀分布\n",
    "#         weights = np.ones(n_samples) / n_samples\n",
    "#\n",
    "#         # 逐步训练弱分类器\n",
    "#         for _ in range(self.n_estimators):\n",
    "#             # 创建并训练基学习器\n",
    "#             learner = self.base_learner_class()\n",
    "#             min_error = learner.fit(X, y, weights)\n",
    "#\n",
    "#             # 防止除零错误\n",
    "#             epsilon = 1e-10\n",
    "#\n",
    "#             # 计算弱分类器权重\n",
    "#             alpha = 0.5 * np.log((1.0 - min_error + epsilon) / (min_error + epsilon))\n",
    "#             learner.alpha = alpha\n",
    "#\n",
    "#             # 获取当前弱分类器的预测\n",
    "#             predictions = learner.predict(X)\n",
    "#\n",
    "#             # 更新样本权重\n",
    "#             weights *= np.exp(-alpha * y * predictions)\n",
    "#             # 归一化权重\n",
    "#             weights /= np.sum(weights)\n",
    "#\n",
    "#             # 保存弱分类器\n",
    "#             self.estimators.append(learner)\n",
    "#\n",
    "#             # 如果错误率为0，提前结束训练\n",
    "#             if min_error == 0:\n",
    "#                 break\n",
    "#\n",
    "#         return self\n",
    "#\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"\n",
    "#         使用AdaBoost集成进行预测\n",
    "#\n",
    "#         参数:\n",
    "#             X: 形状为[n_samples, n_features]的测试特征\n",
    "#\n",
    "#         返回:\n",
    "#             预测的类别标签，值为+1或-1\n",
    "#         \"\"\"\n",
    "#         n_samples = X.shape[0]\n",
    "#         y_pred = np.zeros(n_samples)\n",
    "#\n",
    "#         # 计算所有弱分类器的加权投票\n",
    "#         for learner in self.estimators:\n",
    "#             y_pred += learner.alpha * learner.predict(X)\n",
    "#\n",
    "#         # 返回集成结果的符号\n",
    "#         return np.sign(y_pred)\n",
    "#\n",
    "#     def score(self, X, y):\n",
    "#         \"\"\"\n",
    "#         计算分类准确率\n",
    "#\n",
    "#         参数:\n",
    "#             X: 形状为[n_samples, n_features]的测试特征\n",
    "#             y: 形状为[n_samples]的真实标签\n",
    "#\n",
    "#         返回:\n",
    "#             分类准确率（0到1之间）\n",
    "#         \"\"\"\n",
    "#         y_pred = self.predict(X)\n",
    "#         return np.mean(y_pred == y)\n"
   ],
   "id": "9ec6fb45ed3cc50c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:56.147826Z",
     "start_time": "2025-04-20T12:23:56.145324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_feature = load_feature_data('data.csv')\n",
    "# train_label = load_label_data('targets.csv')\n",
    "#\n",
    "# model = MyAdaBoost()\n",
    "# train_label =np.array([1 if label == 1 else -1 for label in train_label])\n",
    "# model.fit(train_feature, train_label)\n",
    "# y_pred = model.predict(train_feature)\n",
    "# print(\"Predictions:\", y_pred)\n",
    "# print(\"Accuracy:\", model.score(train_feature, train_label))\n",
    "# print(\"Acc:\", accuracy(train_label, y_pred))"
   ],
   "id": "805de9fa35256613",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:23:57.093371Z",
     "start_time": "2025-04-20T12:23:57.075165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class BaseWeakLearner:\n",
    "    \"\"\"弱学习器的基类，定义基本接口\"\"\"\n",
    "    def fit(self, X, y, weights):\n",
    "        \"\"\"使用加权样本进行训练\"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"对样本进行预测\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DecisionStump(BaseWeakLearner):\n",
    "    \"\"\"决策树桩实现 - 一个简单的单层决策树\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_idx = None  # 用于分类的特征索引\n",
    "        self.threshold = None    # 分类阈值\n",
    "        self.polarity = 1        # 极性，决定符号方向（+1 或 -1）\n",
    "        self.alpha = None        # 该分类器在集成中的权重\n",
    "\n",
    "    def fit(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        训练决策树桩，在加权数据上找到最佳分割点\n",
    "\n",
    "        参数:\n",
    "            X: 形状为[n_samples, n_features]的训练特征\n",
    "            y: 形状为[n_samples]的训练标签，值为+1或-1\n",
    "            weights: 样本权重\n",
    "\n",
    "        返回:\n",
    "            min_error: 最小的加权错误率\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        min_error = float('inf')\n",
    "\n",
    "        # 对每个特征尝试找到最佳的阈值和极性\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            thresholds = np.unique(feature_values)\n",
    "\n",
    "            # 对每个可能的阈值进行评估\n",
    "            for threshold in thresholds:\n",
    "                # 尝试两种极性\n",
    "                for polarity in [1, -1]:\n",
    "                    # 预测结果\n",
    "                    predictions = np.ones(n_samples)\n",
    "                    if polarity == 1:\n",
    "                        predictions[feature_values < threshold] = -1\n",
    "                    else:\n",
    "                        predictions[feature_values >= threshold] = -1\n",
    "\n",
    "                    # 计算加权错误率\n",
    "                    misclassified = predictions != y\n",
    "                    error = np.sum(weights[misclassified])\n",
    "\n",
    "                    # 更新找到更好的参数组合\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        self.feature_idx = feature_idx\n",
    "                        self.threshold = threshold\n",
    "                        self.polarity = polarity\n",
    "\n",
    "        return min_error\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"根据选定的特征和阈值预测样本类别\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        feature_values = X[:, self.feature_idx]\n",
    "\n",
    "        # 初始化预测结果\n",
    "        predictions = np.ones(n_samples)\n",
    "\n",
    "        # 应用阈值和极性进行分类\n",
    "        if self.polarity == 1:\n",
    "            predictions[feature_values < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[feature_values >= self.threshold] = -1\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "class LogisticRegression(BaseWeakLearner):\n",
    "    \"\"\"逻辑回归作为基学习器的实现\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.1, n_iterations=1000, tol=1e-10):\n",
    "        \"\"\"\n",
    "        初始化逻辑回归分类器\n",
    "\n",
    "        参数:\n",
    "            learning_rate: 梯度下降的学习率\n",
    "            n_iterations: 最大迭代次数\n",
    "            tol: 收敛阈值，当参数变化小于此值时停止迭代\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.tol = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.alpha = None  # 该分类器在AdaBoost集成中的权重\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Sigmoid激活函数\"\"\"\n",
    "        # 使用截断来避免溢出\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        使用加权样本训练逻辑回归模型\n",
    "\n",
    "        参数:\n",
    "            X: 形状为[n_samples, n_features]的训练特征\n",
    "            y: 形状为[n_samples]的训练标签，值为+1或-1\n",
    "            weights: 样本权重\n",
    "\n",
    "        返回:\n",
    "            error: 加权分类错误率\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # 将y转换为0/1标签用于逻辑回归\n",
    "        y_binary = (y + 1) / 2\n",
    "\n",
    "        # 初始化模型参数\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # 梯度下降优化\n",
    "        for i in range(self.n_iterations):\n",
    "            # 计算线性预测\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            # 计算sigmoid激活后的预测\n",
    "            y_pred = self._sigmoid(linear_pred)\n",
    "\n",
    "            # 计算加权梯度\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y_binary) * weights)\n",
    "            db = (1 / n_samples) * np.sum((y_pred - y_binary) * weights)\n",
    "\n",
    "            # 更新参数\n",
    "            w_prev = self.weights.copy()\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            # 检查收敛\n",
    "            if np.mean(np.abs(self.weights - w_prev)) < self.tol:\n",
    "                break\n",
    "\n",
    "        # 计算加权错误率\n",
    "        y_pred_labels = self.predict(X)\n",
    "        misclassified = y_pred_labels != y\n",
    "        error = np.sum(weights[misclassified])\n",
    "\n",
    "        return error\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        预测样本类别\n",
    "\n",
    "        参数:\n",
    "            X: 形状为[n_samples, n_features]的测试特征\n",
    "\n",
    "        返回:\n",
    "            预测类别，值为+1或-1\n",
    "        \"\"\"\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self._sigmoid(linear_pred)\n",
    "\n",
    "        # 将概率转换为二分类标签(+1/-1)\n",
    "        return np.where(y_pred >= 0.5, 1, -1)\n",
    "\n",
    "\n",
    "class MyAdaBoost:\n",
    "    \"\"\"AdaBoost分类器实现\"\"\"\n",
    "\n",
    "    def __init__(self, base_learner_class=DecisionStump, n_estimators=50, **base_params):\n",
    "        \"\"\"\n",
    "        初始化AdaBoost分类器\n",
    "\n",
    "        参数:\n",
    "            base_learner_class: 基学习器类（默认为决策树桩）\n",
    "            n_estimators: 弱分类器的数量\n",
    "            base_params: 传递给基学习器的参数\n",
    "        \"\"\"\n",
    "        self.base_learner_class = base_learner_class\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_params = base_params\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        训练AdaBoost分类器\n",
    "\n",
    "        参数:\n",
    "            X: 形状为[n_samples, n_features]的训练特征\n",
    "            y: 形状为[n_samples]的训练标签，值为+1或-1\n",
    "\n",
    "        返回:\n",
    "            self: 训练后的分类器\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # 初始化样本权重为均匀分布\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "        # 逐步训练弱分类器\n",
    "        for _ in range(self.n_estimators):\n",
    "            # 创建并训练基学习器\n",
    "            learner = self.base_learner_class(**self.base_params)\n",
    "            min_error = learner.fit(X, y, weights)\n",
    "\n",
    "            # 防止除零错误或错误率太高的情况\n",
    "            epsilon = 1e-10\n",
    "            if min_error >= 0.5:\n",
    "                # 如果错误率大于0.5，就放弃这个分类器\n",
    "                continue\n",
    "\n",
    "            # 计算弱分类器权重\n",
    "            alpha = 0.5 * np.log((1.0 - min_error + epsilon) / (min_error + epsilon))\n",
    "            learner.alpha = alpha\n",
    "\n",
    "            # 获取当前弱分类器的预测\n",
    "            predictions = learner.predict(X)\n",
    "\n",
    "            # 更新样本权重\n",
    "            weights *= np.exp(-alpha * y * predictions)\n",
    "            # 归一化权重\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            # 保存弱分类器\n",
    "            self.estimators.append(learner)\n",
    "\n",
    "            # 如果错误率为0，提前结束训练\n",
    "            if min_error == 0:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        使用AdaBoost集成进行预测\n",
    "\n",
    "        参数:\n",
    "            X: 形状为[n_samples, n_features]的测试特征\n",
    "\n",
    "        返回:\n",
    "            预测的类别标签，值为+1或-1\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        y_pred = np.zeros(n_samples)\n",
    "\n",
    "        # 确保至少有一个基学习器\n",
    "        if not self.estimators:\n",
    "            return np.ones(n_samples)\n",
    "\n",
    "        # 计算所有弱分类器的加权投票\n",
    "        for learner in self.estimators:\n",
    "            y_pred += learner.alpha * learner.predict(X)\n",
    "\n",
    "        # 返回集成结果的符号\n",
    "        return np.sign(y_pred)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        计算分类准确率\n",
    "\n",
    "        参数:\n",
    "            X: 形状为[n_samples, n_features]的测试特征\n",
    "            y: 形状为[n_samples]的真实标签\n",
    "\n",
    "        返回:\n",
    "            分类准确率（0到1之间）\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n"
   ],
   "id": "ff533f17d387a4bd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T12:24:33.535292Z",
     "start_time": "2025-04-20T12:24:00.227265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_feature = load_feature_data('data.csv')\n",
    "train_label = load_label_data('targets.csv')\n",
    "\n",
    "model = MyAdaBoost(n_estimators=10)\n",
    "train_label =np.array([1 if label == 1 else -1 for label in train_label])\n",
    "model.fit(train_feature, train_label)\n",
    "y_pred = model.predict(train_feature)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"Accuracy:\", model.score(train_feature, train_label))\n",
    "print(\"Acc:\", accuracy(train_label, y_pred))\n",
    "\n",
    "model = MyAdaBoost(base_learner_class=LogisticRegression(learning_rate=0.1, n_iterations=100, tol=1e-12), n_estimators=100, learning_rate=0.1)\n",
    "train_label =np.array([1 if label == 1 else -1 for label in train_label])\n",
    "model.fit(train_feature, train_label)\n",
    "y_pred = model.predict(train_feature)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"Accuracy:\", model.score(train_feature, train_label))\n",
    "print(\"Acc:\", accuracy(train_label, y_pred))"
   ],
   "id": "20f67d385175415e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 正在执行函数: load_feature_data\n",
      "[DEBUG] 正在执行函数: load_label_data\n",
      "Predictions: [ 1.  1.  1. ... -1. -1. -1.]\n",
      "Accuracy: 0.9114130434782609\n",
      "[DEBUG] 正在执行函数: accuracy\n",
      "Acc: 0.9114130434782609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAcc:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy(train_label, y_pred))\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m MyAdaBoost(base_learner_class\u001B[38;5;241m=\u001B[39mLogisticRegression(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, n_iterations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, tol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-12\u001B[39m), n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m train_label \u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m label \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m train_label])\n\u001B[1;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(train_feature, train_label)\n\u001B[1;32m     15\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(train_feature)\n",
      "Cell \u001B[0;32mIn[10], line 13\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAcc:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy(train_label, y_pred))\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m MyAdaBoost(base_learner_class\u001B[38;5;241m=\u001B[39mLogisticRegression(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, n_iterations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, tol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-12\u001B[39m), n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m train_label \u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m label \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m train_label])\n\u001B[1;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(train_feature, train_label)\n\u001B[1;32m     15\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(train_feature)\n",
      "Cell \u001B[0;32mIn[10], line 13\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAcc:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy(train_label, y_pred))\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m MyAdaBoost(base_learner_class\u001B[38;5;241m=\u001B[39mLogisticRegression(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m, n_iterations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, tol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-12\u001B[39m), n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m---> 13\u001B[0m train_label \u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m label \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m train_label])\n\u001B[1;32m     14\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(train_feature, train_label)\n\u001B[1;32m     15\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(train_feature)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1065\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1220\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1219\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1220\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1235\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1232\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1234\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1235\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1239\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
